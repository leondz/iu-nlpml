# Natural Language Processing and Machine Learning

Great thanks to Rob Gaizauskas (U Sheffield), Noah Smith (U Washington), Chris Manning and the CS224n staffers (Stanford)

# Info

This is a computer science course, expecting some knowledge of machine learning

## Schedule
* Monday 12.10-15.40 rm 103
* Tuesday 12.10-15.40 rm 306
* Wednesday 9.00-10.30, 14.10-15.40 rm 102
* Thursday 14.10-17.15 rm 316
* Friday 9.00-12.05 rm 103

## Assessment

Three assignments, one per week; they are worth 30%, 30% and 40% of the final grade.

## Reading

* [Manning and Schutze](https://www.cs.vassar.edu/~cs366/docs/Manning_Schuetze_StatisticalNLP.pdf)
* 

# Week 1

## Day 1
* Introduction to the course [slides](1a - admin, intro.odp)
* Outline of NLP [slides](1b - nlp outline.pdf)
* What is NLP - details [slides](1c - what is nlp - details.pdf)
* What is NLP - challenges [slides](1d - what is nlp - challenges and brief tour.pdf)
* Levels of linguistics [slides](1e - levels of linguistics, syntax phonology etc.pdf)

## Day 2
* Tokenization [slides 1f](1f - tokenization.pdf), [slides 1g](1g - tokenization approaches.pdf)
* Exercise 1: sentiment classifier [slides](2c - ngrams exercise (movie reviews).pdf), [notebook](reviews.ipynb)

## Day 3
* Word embeddings [slides stanford](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture01-wordvecs1.pdf), [notes stanford](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf), [slides yoav](https://www.slideshare.net/mlreview/yoav-goldberg-word-embeddings-what-how-and-whither) 
* Lexical semantics [slides](3a - lexical semantics.pdf)

## Day 4
* Word similarity [slides](3c - word similarity.pdf)
* Part of Speech tagging [slides](2d - pos tags.pdf), [extra slides](4d-markov.pdf) 


## Day 5
* Word embeddings [slides](9b-embeddings.pdf)
* LSTM [slides](11a-lstm.pdf)
* [Assignment 1](assignment%201.pdf) - due 20 Sept

# Week 2

## Day 1
* Perceptrons, [slides](6a-nnets and dl.pdf)
* Backpropagation, [slides](6c-multilayer.pdf)

## Day 2
* Backpropagation and word encoding, [slides](lstm2.pdf), 
* Autoencoding [page](https://blog.keras.io/building-autoencoders-in-keras.html) 
* Exercise - autoencoder for text; see the Keras blog, the slides from earlier in the course, and this [tutorial](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)

## Day 4
* Statistical machine translation [slides](10c-smt.pdf)
* Sequence-to-sequence learning with Neural Machine translation [blog](https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/), attention [blog](https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39)
* Neural language modelling [slides](neural-lm.pdf)

## Day 5
* Dialogue [slides](10a.pdf)

## Project
* Some sample ideas & recommendations for the project format [slides](project.pdf)

# Week 3

## Day 1
* Project help / presentations

## Day 2
* CRFsuite: a brief tour [slides](4e-crfsuite.pdf), [data](4f.tar.gz)
* Social media processing [intro](8a.pdf), [language](8b.pdf)
