# Natural Language Processing and Machine Learning

Great thanks to Rob Gaizauskas (U Sheffield), Noah Smith (U Washington), Chris Manning and the CS224n staffers (Stanford), GATE, Manuel Ciosici, Philipp Koehn

# Info

This is a computer science course, expecting some knowledge of machine learning and good programming skills.

## Schedule
* Monday 09.00-13.40
* Tuesday 17.20-18.50
* Wednesday 09.00-12.05
* Thursday 09.00-10.30, 15.45-17.15
* Friday 10.35-13.40

## Assessment

Three assignments, one per week; they are worth 30%, 30% and 40% of the final grade.

## Reading

* [Manning and Schutze](https://www.cs.vassar.edu/~cs366/docs/Manning_Schuetze_StatisticalNLP.pdf)
* 

# Week 1

## Day 1
* Introduction to the course [slides](1a - admin, intro.odp)
* Outline of NLP [slides](1b - nlp outline.pdf)
* What is NLP - details [slides](1c - what is nlp - details.pdf)
* What is NLP - challenges [slides](1d - what is nlp - challenges and brief tour.pdf)
* Levels of linguistics [slides](1e - levels of linguistics, syntax phonology etc.pdf)

## Day 2
* Tokenization [slides 1f](1f - tokenization.pdf), [slides 1g](1g - tokenization approaches.pdf)
* Exercise 1: sentiment classifier [slides](2c - ngrams exercise (movie reviews).pdf), [notebook](reviews.ipynb)

## Day 3
* Word embeddings [slides stanford](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture01-wordvecs1.pdf), [notes stanford](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf), [slides yoav](https://www.slideshare.net/mlreview/yoav-goldberg-word-embeddings-what-how-and-whither) 
* Lexical semantics [slides](3a - lexical semantics.pdf)

## Day 4
* Word similarity [slides](3c - word similarity.pdf)
* Part of Speech tagging [slides](2d - pos tags.pdf), [extra slides](4d-markov.pdf) 

## Day 5
* Word embeddings [slides](9b-embeddings.pdf)
* Language models and smoothing [slides](smoothing.pdf)
* [Assignment 1](assn1-hmm-emb.pdf): HMM Tagger -or- Embedding induction

# Week 2

## Day 1
* Perceptrons, [slides](6a-nnets and dl.pdf) [notebook](https://colab.research.google.com/drive/1eELLPBIZEkS7UYiPjfPaqdESLHS6uYk6)
* Backpropagation, [slides](6c-multilayer.pdf)
* Autoencoding [page](https://blog.keras.io/building-autoencoders-in-keras.html) [AE notebook](https://colab.research.google.com/drive/1P1n8LEIzKZSrlvIWx3YdwdNkgFu9facK) [Denoising AE notebook](https://colab.research.google.com/drive/1erwsMQvq3nqjIcK5rHxeWWWBBPZSKtxC)


## Day 2
* LSTM [slides](11a-lstm.pdf)
* Backpropagation and word encoding, [slides](lstm2.pdf), 
* Exercise - autoencoder for text; see the Keras blog, the slides from earlier in the course, and this [tutorial](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)

## Day 4
* Statistical machine translation [slides](10c-smt.pdf)
* Sequence-to-sequence learning with Neural Machine translation [blog](https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/), attention [blog](https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39)
* Neural language modelling [slides](neural-lm.pdf)

## Day 5
* Dialogue [slides](10a.pdf)
* Assignment: [NER](assignment%203.pdf)

# Week 3

## Day 1
* Project help / presentations

## Day 2
* CRFsuite: a brief tour [slides](4e-crfsuite.pdf), [data](4f.tar.gz)
* Social media processing [intro](8a.pdf), [language](8b.pdf)

## Day 3
* Named entity recognition [slides](8c.pdf) with live examples from [displaCy](https://explosion.ai/demos/displacy-ent)

## Day 4
* Advanced clustering, word clustering [slides](clustering_slides.pdf)

## Day 5
* Course recap
* Transfer learning tutorial [NAACL slides](https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc)
* Transformer [tutorial](http://jalammar.github.io/illustrated-transformer/)


## Project
* Some sample ideas & recommendations for the project format [slides](project.pdf)
